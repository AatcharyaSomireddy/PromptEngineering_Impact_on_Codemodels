Prompt Engineering Impact on Generative Code Models
End-to-end benchmark to study how different prompt strategies affect Java code generation quality, speed, and token usage for openâ€‘source code LLMs (StarCoderâ€‘1B, CodeT5â€‘Small).

ğŸš€ Features
Multiple prompt strategies:

Zeroâ€‘Shot

Chainâ€‘ofâ€‘Thought (CoT)

Personaâ€‘Based

Fewâ€‘Shot

Templateâ€‘Based

Multiple Java coding problems (recursion, trees, graphs, caching, matrix operations, etc.).

Supports openâ€‘source models (e.g., bigcode/starcoder, Salesforce/codet5-small).

Automatic:

Experiment orchestration (problem Ã— model Ã— strategy)

Execution time measurement

Token counting

Aggregated CSV reports

Publicationâ€‘ready charts (bar plots + heatmaps)

Reproducible, configurationâ€‘driven design (JSON/YAML).

ğŸ“‚ Project Structure
.
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ ai_models_only.json â€“ which models + strategies to run
â”‚ â””â”€â”€ problems_config.json â€“ which problem set to use
â”œâ”€â”€ problems/
â”‚ â””â”€â”€ *.json â€“ problem descriptions + constraints
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ benchmark_runner.py â€“ main experiment loop
â”‚ â”‚ â”œâ”€â”€ prompt_manager.py â€“ prompt templates and strategies
â”‚ â”‚ â””â”€â”€ model_interface.py â€“ StarCoder / CodeT5 wrappers
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ results_io.py â€“ CSV/JSON readâ€“write
â”‚ â”‚ â”œâ”€â”€ visualization.py â€“ plots (bar charts, heatmaps)
â”‚ â”‚ â””â”€â”€ reporting.py â€“ text summary (key insights)
â”œâ”€â”€ outputs/
â”‚ â”œâ”€â”€ csv/ â€“ raw metrics per run
â”‚ â”œâ”€â”€ figures/ â€“ PNG charts for each problem
â”‚ â””â”€â”€ reports/ â€“ text summaries
â”œâ”€â”€ main.py â€“ CLI entry point for running benchmarks
â””â”€â”€ README.md

(Adjust names/paths to match your actual repo.)

âœ… Requirements
Python 3.10+

A GPU is recommended (but small runs can work on CPU).

Typical Python dependencies:

pip install -r requirements.txt

Example requirements.txt:

torch
transformers
accelerate
datasets
pandas
numpy
matplotlib
seaborn
pyyaml
tqdm

âš™ï¸ Quick Start
Clone the repository

git clone https://github.com/<your-username>/prompt-engineering-java-benchmark.git
cd prompt-engineering-java-benchmark

Install dependencies

pip install -r requirements.txt

Configure models and strategies

Edit config/ai_models_only.json (example):

{
"models": ["starcoder-1b", "codet5-small"],
"strategies": ["zero_shot", "cot", "persona", "few_shot", "template"],
"problem_set": "algorithms_all",
"max_concurrent_requests": 1
}

Run the benchmark

python main.py --problem-set algorithms_all --config config/ai_models_only.json

View results

Raw metrics: outputs/csv/

Charts (per problem): outputs/figures/

Text summaries: outputs/reports/

ğŸ§  Prompt Strategies
All strategies are implemented as reusable templates in src/core/prompt_manager.py:

Zeroâ€‘Shot â€“ minimal prompt with only the problem description and constraints.

Chainâ€‘ofâ€‘Thought (CoT) â€“ stepâ€‘byâ€‘step reasoning instructions before requesting code.

Personaâ€‘Based â€“ model acts as a senior Java developer producing clean, efficient code.

Fewâ€‘Shot â€“ 2â€“3 example problemâ€“solution pairs before the target problem.

Templateâ€‘Based â€“ Java skeleton with TODO comments for the model to fill in.

You can modify or add strategies by editing templates in prompt_manager.py.

ğŸ§ª Evaluation Metrics
For each (problem, model, strategy) combination, the framework records:

Generation time (seconds) â€“ latency from prompt to code completion

Token count â€“ number of output tokens (verbosity / potential API cost)

Generation success â€“ whether the model produced a nonâ€‘empty output without errors

Aggregated statistics â€“ perâ€‘strategy and perâ€‘model averages, plus performance range

These metrics are stored in CSV files and used to generate:

Strategy performance bar charts (lower time is better)

Model performance bar charts

Token efficiency bar charts (lower tokens is better)

Model Ã— strategy heatmaps (execution time encoded as color)

ğŸ“Š Example Command and Output
python main.py --problem-set bst_problems --config config/ai_models_only.json

Produces, for example:

outputs/csv/bst_problems_metrics.csv

outputs/figures/bst_problems_strategy_performance.png

outputs/figures/bst_problems_model_vs_strategy_heatmap.png

outputs/reports/bst_problems_summary.txt

Each summary report typically includes:

Fastest strategy

Fastest model

Most tokenâ€‘efficient strategy

Overall generation success rate

Performance range between best and worst combinations

ğŸ”§ Configuration
Main CLI options:

python main.py
--problem-set <name>
--config config/ai_models_only.json
--out-dir outputs/

Key config fields:

models â€“ list of model IDs (Hugging Face names or local aliases)

strategies â€“ list of strategy identifiers (must match enum/keys in code)

problem_set â€“ which logical problem group to run (must exist in problem config)

max_concurrent_requests â€“ concurrency limit (1 recommended for stable timing)

ğŸ“š Extending the Benchmark
Add a new problem:

Create a JSON file in problems/, e.g. problems/valid_parentheses.json:

{
"id": "valid_parentheses",
"title": "Valid Parentheses",
"description": "Check if a string of brackets is valid.",
"constraints": "Only characters '()[]{}'; return boolean.",
"examples": [
{ "input": ""()[]{}"", "output": "true" },
{ "input": ""(]"", "output": "false" }
]
}

Register it in config/problems_config.json under the appropriate problem set.

Add a new model:

Implement or reuse a wrapper in src/core/model_interface.py.

Add the model name to the models list in your config file.

Add a new strategy:

Define a new strategy template in src/core/prompt_manager.py.

Add its name to the strategy enum/strategy registry.

Reference it in your configuration file under strategies.

ğŸ§¾ Academic Use and Citation
If you use this benchmark in academic work, please cite:

Aatcharya Somireddy,Thotamsetty Sreehitha, Srirangam Varshitha, â€œEmpirical Evaluation of Prompt Engineering Strategies for Openâ€‘Source Neural Code Generation Models,â€ 2025.

(Adjust citation details once your paper is published.)



